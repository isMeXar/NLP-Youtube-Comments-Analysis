{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jpkrajewski/NLP-youtube-analysis/blob/main/NLP_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwK5-9FIB-lu"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "NLP stands for Natural Language Processing. It is a subfield of artificial intelligence and linguistics that focuses on the interaction between computers and human language. NLP involves developing algorithms, models, and techniques that enable computers to understand, interpret, and generate human language in a way that is meaningful and useful.\n",
    "\n",
    "The primary goal of NLP is to bridge the gap between human language and computer language, allowing machines to process, analyze, and extract information from textual data. NLP encompasses a wide range of tasks and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1kiO9kACE6s"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7QG7sxmoCIvN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTfaCIzdCLPA"
   },
   "source": [
    "## Importing the dataset\n",
    "\n",
    "Generic Sentiment | Multidomain Sentiment Dataset\n",
    "50K sentiments merged from multiple domain (Yelp, Twitter, Mobile reviews)\n",
    "\n",
    "https://www.kaggle.com/datasets/akgeni/generic-sentiment-multidomain-sentiment-dataset\n",
    "\n",
    "**Context**\n",
    "\n",
    "We find sentiment dataset pertaining to a domain. To have a general sense of sentiment we need to understand the sentiment semantics.\n",
    "\n",
    "**Content**\n",
    "\n",
    "Combined Mobile reviews, Twitter sentiment, Yelp review, Toxic reviews and few more to cover multiple domain of sentiment analysis.\n",
    "\n",
    "* 0->Negative\n",
    "* 1->Neutral\n",
    "* 2->Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UCK6vQ5QCQJe"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./dataset/generic_sentiment_dataset_50k.csv')\n",
    "features = dataset.iloc[:, 1].values\n",
    "labels = dataset.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7epTln2NOZsD",
    "outputId": "de962f17-099e-4dc4-c85e-8096e0b96d84"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>good mobile. battery is 5000 mah is very big. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>Overall in hand ecpirience is quite good matt ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>1. Superb Camera,\\n2. No lag\\n3. This is my fi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Bigger size of application names doesn't allow...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Just a hype of stock android which is not flaw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  label\n",
       "0  positive  good mobile. battery is 5000 mah is very big. ...      2\n",
       "1  positive  Overall in hand ecpirience is quite good matt ...      2\n",
       "2  positive  1. Superb Camera,\\n2. No lag\\n3. This is my fi...      2\n",
       "3  positive  Bigger size of application names doesn't allow...      2\n",
       "4  negative  Just a hype of stock android which is not flaw...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qekztq71CixT"
   },
   "source": [
    "## Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8u_yXh9dCmEE"
   },
   "outputs": [],
   "source": [
    "# In Natural Language Processing (NLP), text preprocessing plays a crucial role in preparing textual data for analysis.\n",
    "\n",
    "# The code  aims to clean and normalize the text data,\n",
    "# reducing noise and simplifying subsequent NLP analysis.\n",
    "\n",
    "# Preprocessing is crucial for improving the quality and effectiveness of NLP models and algorithms,\n",
    "# as it helps standardize the text and remove irrelevant information,\n",
    "# allowing the focus to be on the meaningful aspects of the text that are relevant to the task at hand.\n",
    "\n",
    "import re\n",
    "processed_features = []\n",
    "for sentence in features:\n",
    "\n",
    "  # Remove all the special characters\n",
    "  processed_feature = re.sub(r'\\W', ' ', str(sentence))\n",
    "\n",
    "  # remove all single characters\n",
    "  processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "  # Remove single characters from the start\n",
    "  processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "  # Substituting multiple spaces with single space\n",
    "  processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "  # Removing prefixed 'b'\n",
    "  processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "  # Converting to Lowercase\n",
    "  processed_feature = processed_feature.lower()\n",
    "  processed_features.append(processed_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLqmAkANCp1-"
   },
   "source": [
    "## Creating the Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lit7XRDbPjgR"
   },
   "source": [
    "Utilizing the NLTK (Natural Language Toolkit) library and the scikit-learn library (specifically the TfidfVectorizer class) to perform feature extraction using the TF-IDF (Term Frequency-Inverse Document Frequency) approach.\n",
    "\n",
    "The resulting processed_features will contain the numerical feature vectors representing the preprocessed text data, where each feature vector corresponds to a document (in this case, a processed comment). The TF-IDF approach assigns weights to words based on their frequency in a document and their inverse frequency across the entire corpus, allowing the importance of each word to be captured in the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qroF7XcSCvY3",
    "outputId": "37c1e3ee-05df-4602-9193-92f00513fedd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1500, stop_words=stopwords.words('english'))\n",
    "processed_features = vectorizer.fit_transform(processed_features).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH_VjgPzC2cd"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qQXYM5VzDDDI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkIq23vEDIPt"
   },
   "source": [
    "## Training the RandomForestClassifer model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "DS9oiDXXDRdI",
    "outputId": "e9b31783-db3c-4506-a424-736de145f244"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=80, random_state=0)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JaRM7zXDWUy"
   },
   "source": [
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iif0CVhFDaMp"
   },
   "outputs": [],
   "source": [
    "rf_predictions = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoMltea5Dir1"
   },
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xj9IU6MxDnvo",
    "outputId": "8b716921-5d76-48b7-c42d-875f43c0ae82"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "print(accuracy_score(y_test, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DjB4P6czwZQ"
   },
   "source": [
    "## Training the KNN model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "ORD8rYWbzwZZ",
    "outputId": "5b04ae9c-dd12-4ae5-c66b-18e6d6ba69c3"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-J1FmBtzwZa"
   },
   "source": [
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaREzvMwzwZa"
   },
   "outputs": [],
   "source": [
    "knn_predictions = knn_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c65AOwHzwZa"
   },
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PItC-8RzwZa",
    "outputId": "6568eeb1-8487-4a7c-e117-2925f757532f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test, knn_predictions))\n",
    "print(classification_report(y_test, knn_predictions))\n",
    "print(accuracy_score(y_test, knn_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7umtTeD7wmr"
   },
   "source": [
    "## Training the Bayes model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "dHXBgr5K7wmx",
    "outputId": "859e985c-956c-43f9-9091-665f69d69e27"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_classifier = GaussianNB()\n",
    "gnb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OodFtETF7wmx"
   },
   "source": [
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMjxDlqR7wmx"
   },
   "outputs": [],
   "source": [
    "gnb_predictions = gnb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utbiQIPw7wmx"
   },
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TBsycr37wmx",
    "outputId": "8a4ef1ec-9b5e-4f8a-9b54-98d0c5c8a8cf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test, gnb_predictions))\n",
    "print(classification_report(y_test, gnb_predictions))\n",
    "print(accuracy_score(y_test, gnb_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DecisionTree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree_classifier = DecisionTreeClassifier()\n",
    "dtree_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_predictions = dtree_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, dtree_predictions))\n",
    "print(classification_report(y_test, dtree_predictions))\n",
    "print(accuracy_score(y_test, dtree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Logistic Regression OVR (One-vs-Rest) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lreg_classifier = LogisticRegression(multi_class='ovr')\n",
    "lreg_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_predictions = lreg_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, lreg_predictions))\n",
    "print(classification_report(y_test, lreg_predictions))\n",
    "print(accuracy_score(y_test, lreg_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch for Tunning model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy on Test Set: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YconfS008Z30"
   },
   "source": [
    "## Downloading the model to deploy in production\n",
    "\n",
    "The RandomForestClassifer has the best accuracy, so I am choosing this model for application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6dplQtm8nf3",
    "outputId": "7ec8974c-d3ab-4fdb-ca73-d096324ebb58"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_classifier, './app/finalized_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUZiApGbCk9y",
    "outputId": "3dd5df4f-1265-404c-e9b2-96b1422d7528"
   },
   "outputs": [],
   "source": [
    "joblib.dump(vectorizer, './app/vectorizer.sav')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "history_visible": true,
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
